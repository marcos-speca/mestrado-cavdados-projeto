{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontra e Carrega os Processos por CNPJ ou Nome da Parte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as Bibliotecas\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pymongo\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros Iniciais TJSP Esaj\n",
    "url_base = \"https://esaj.tjsp.jus.br/cpopg/search.do?\"\n",
    "\n",
    "#Parâmetros de Busca \n",
    "busca_nome = \"\".replace(' ','+')\n",
    "busca_cnpj = \"\"  \"61074175000138\"\n",
    "cod_empresa = \"MAPFRE\"\n",
    "dados = []\n",
    "\n",
    "if busca_cnpj: \n",
    "    url_pesquisa = url_base + \"cbPesquisa=DOCPARTE&dadosConsulta.valorConsulta=\" + busca_cnpj\n",
    "else: \n",
    "    url_pesquisa = url_base + \"cbPesquisa=NMPARTE&dadosConsulta.valorConsulta=\" + busca_nome\n",
    "\n",
    "url_pesquisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexão com o Banco de Dados\n",
    "user = \"\"\n",
    "client = pymongo.MongoClient(\"mongodb+srv://\" + user + \"@cluster0.kqbxfp4.mongodb.net/?retryWrites=true&w=majority\")\n",
    "db = client['proc-jud']\n",
    "col_processos = db['processo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de Foros do TJSP\n",
    "lista_foros = [1, 2, 3, 4, 5, 6,7, 8, 9, 10, 11, 12,13, 14, 15, 16, 17, 18,19, 20, 21, 22, 24, 25,28, 30, 32, 35, 37, 38,40, 42, 45, 47, 48, 50,52, 53, 58, 59, 60, 62,\n",
    "63, 66, 67, 68, 69, 70,71, 72, 73, 75, 76, 77,79, 80, 81, 82, 83, 84,85, 86, 87, 88, 89, 90,91, 93, 94, 95, 97, 99,100, 101, 102, 103, 104, 106,108, 111, 114, 115, 116, 118,\n",
    "120, 123, 125, 126, 127, 128,129, 132, 136, 137, 140, 142,144, 145, 146, 150, 152, 153,156, 157, 159, 160, 161, 165,168, 169, 172, 176, 177, 180,185, 187, 189, 191, 196, 197,\n",
    "198, 200, 201, 204, 205, 210,213, 218, 219, 220, 222, 223,224, 229, 233, 236, 238, 240,242, 244, 246, 247, 248, 252,257, 262, 263, 264, 266, 268,269, 270, 271, 272, 274, 275,\n",
    "278, 279, 280, 281, 282, 283,286, 288, 291, 292, 294, 296,297, 299, 300, 301, 302, 306,309, 311, 312, 315, 318, 319,320, 322, 323, 326, 333, 334,337, 338, 341, 344, 346, 347,\n",
    "348, 352, 355, 356, 357, 358,360, 361, 362, 363, 366, 368,369, 370, 372, 374, 382, 383,390, 394, 396, 397, 400, 404,405, 407, 408, 411, 412, 414,415, 416, 417, 418, 420, 424,\n",
    "426, 428, 430, 431, 434, 435,438, 439, 441, 443, 444, 445,447, 449, 450, 451, 452, 453,456, 457, 458, 459, 462, 464,466, 470, 471, 472, 474, 477,480, 481, 482, 483, 484, 486,\n",
    "488, 491, 493, 495, 498, 505,506, 510, 511, 512, 515, 516,523, 526, 531, 533, 534, 538,539, 541, 543, 547, 549, 553,554, 562, 563, 564, 565, 566,568, 572, 575, 576, 577, 579,\n",
    "581, 582, 584, 586, 587, 588,589, 590, 595, 596, 597, 601,602, 604, 606, 607, 609, 614,615, 619, 620, 624, 625, 627,629, 634, 637, 638, 642, 646,648, 650, 651, 653, 654, 655,\n",
    "659, 660, 663, 664, 665, 666,667, 668, 669, 670, 671, 672,673, 674, 675, 676, 677, 678,679, 680, 681, 682, 683, 684,685, 686, 687, 688, 689, 690,691, 692, 693, 694, 695, 696,\n",
    "697, 698, 699, 700, 701, 702,703, 704, 705, 706, 707, 708,709, 710, 990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_processo(linha):\n",
    "    json_str = '{\"processo\":' + json.dumps(linha) + '}'\n",
    "    col_processos.find_one_and_update({'processo.nro_processo':linha['nro_processo']},{'$set':json.loads(json_str)},upsert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping_foro(foro):\n",
    "   # Monta a URL de Busca\n",
    "   url_busca = url_pesquisa + \"&cdForo=\" + str(foro)\n",
    "   \n",
    "   # Tenta fazer o request\n",
    "   try:\n",
    "      html = requests.get(url_busca).content\n",
    "   except:\n",
    "      print(\"Não foi possível carregar a URL\")\n",
    "   \n",
    "   # Faz o parser do HTML\n",
    "   soup = BeautifulSoup(html, 'html.parser')\n",
    "   \n",
    "   # Se retornou muito ou não retornou resultados, a página exibe mensagem \n",
    "   mensagem = soup.find('td',attrs={'id':'mensagemRetorno'})\n",
    "\n",
    "   # Em alguns casos, se tive um processo só, ele retorna o processo\n",
    "   processo_unico = soup.find('span',attrs={'id':'numeroProcesso'})\n",
    "\n",
    "   # Caso retorne mais do que um processo, ele mostra a quantidades (Util para saber as páginas)\n",
    "   qtde_processos = soup.find(\"span\",attrs={'id':'contadorDeProcessos'})\n",
    "\n",
    "   # Se não encontrou\n",
    "   if mensagem: \n",
    "      mensagem = mensagem.text.strip()\n",
    "      print(\"Scrapping do foro: \" + str(foro) + \" mensagem: \" + mensagem)\n",
    "      \n",
    "   # No caso de processo Unico, faz o scrapping\n",
    "   elif processo_unico:\n",
    "             \n",
    "      nro_processo = processo_unico.text.strip()\n",
    "      print(\"Scrapping do foro: \" + str(foro) + \" processos: 1\")\n",
    "\n",
    "      link_processo = ''\n",
    "      nome_parte = ''\n",
    "      classe_processo = soup.find('span',attrs={'id':'classeProcesso'}).text.strip()\n",
    "      assunto_processo = soup.find('span',attrs={'id':'assuntoProcesso'})\n",
    "      \n",
    "      if assunto_processo: \n",
    "         assunto_processo = assunto_processo.text.strip()\n",
    "      \n",
    "      data_local_distribuicao = soup.find('div',attrs={'id':'dataHoraDistribuicaoProcesso'}).text.strip()\n",
    "\n",
    "      \n",
    "      linha = dict({\"nro_processo\": nro_processo, \"link_processo\":link_processo, \"nome_parte\":nome_parte, \"classe_processo\":classe_processo, \"assunto_processo\":assunto_processo,\"data_local_distribuicao\":data_local_distribuicao,\"cod_empresa\":cod_empresa})\n",
    "      \n",
    "      if linha:\n",
    "         dados.append([nro_processo, link_processo,nome_parte,classe_processo,assunto_processo,data_local_distribuicao,cod_empresa])\n",
    "         carrega_processo(linha)\n",
    "\n",
    "   # Se tem varias páginas\n",
    "   elif qtde_processos:\n",
    "      \n",
    "      qtde = qtde_processos.text.strip()\n",
    "      qtde = ''.join(c if c.isdigit() else '' for c in qtde)\n",
    "\n",
    "      print(\"Scrapping do foro: \" + str(foro) + \" processos: \" + str(qtde))\n",
    "      \n",
    "      paginas = round(int(qtde)/25)\n",
    "      \n",
    "      # Abre cada página\n",
    "      for pagina in range(1,paginas+1): \n",
    "         url_pagina = url_busca + \"&paginaConsulta=\" + str(pagina)\n",
    "         \n",
    "         try:\n",
    "            html_pagina = requests.get(url_pagina).content\n",
    "         except:\n",
    "            print(\"Não foi possível carregar a URL\")\n",
    "\n",
    "         soup_pagina = BeautifulSoup(html_pagina, 'html.parser')\n",
    "\n",
    "         processo_unico_pagina = soup_pagina.find('span',attrs={'id':'numeroProcesso'})\n",
    "         tabela = soup_pagina.find(\"div\",attrs={'id':'listagemDeProcessos'})  \n",
    "\n",
    "         # No caso de processo Unico na página, faz o scrapping\n",
    "         if processo_unico_pagina:\n",
    "                  \n",
    "            nro_processo = processo_unico_pagina.text.strip()\n",
    "            link_processo = ''\n",
    "            nome_parte = ''\n",
    "            classe_processo = soup.find('span',attrs={'id':'classeProcesso'}).text.strip()\n",
    "            assunto_processo = soup.find('span',attrs={'id':'assuntoProcesso'})\n",
    "            \n",
    "            if assunto_processo: \n",
    "               assunto_processo = assunto_processo.text.strip()\n",
    "            \n",
    "            data_local_distribuicao = soup.find('div',attrs={'id':'dataHoraDistribuicaoProcesso'}).text.strip()\n",
    "\n",
    "            linha = dict({\"nro_processo\":nro_processo, \"link_processo\":link_processo, \"nome_parte\":nome_parte, \"classe_processo\":classe_processo, \"assunto_processo\":assunto_processo,\"data_local_distribuicao\":data_local_distribuicao,\"cod_empresa\":cod_empresa})\n",
    "            \n",
    "            if linha:\n",
    "               dados.append([nro_processo, link_processo,nome_parte,classe_processo,assunto_processo,data_local_distribuicao,cod_empresa])\n",
    "               carrega_processo(linha)\n",
    "         \n",
    "         # Se não faz o scrapping a lista de processos da página\n",
    "         elif tabela:\n",
    "\n",
    "            processos_pagina = tabela.find_all('div',attrs={'class':'row unj-ai-c home__lista-de-processos'})\n",
    "\n",
    "            for proc in processos_pagina:\n",
    "               nro_processo = proc.find('a',attrs={'class':'linkProcesso'}).text.strip()\n",
    "               link_processo = proc.find('a',attrs={'class':'linkProcesso'}).attrs['href'].strip()\n",
    "               try: \n",
    "                  nome_parte = proc.find('div',attrs={'class':'unj-base-alt nomeParte'}).text.strip()\n",
    "               except:\n",
    "                  pass\n",
    "               classe_processo = proc.find('div',attrs={'class':'classeProcesso'}).text.strip()\n",
    "               assunto_processo = proc.find('div',attrs={'class':'assuntoPrincipalProcesso'}).text.strip()\n",
    "               data_local_distribuicao = proc.find('div',attrs={'class':'dataLocalDistribuicaoProcesso'}).text.strip()\n",
    "\n",
    "               linha = dict({\"nro_processo\":nro_processo, \"link_processo\":link_processo, \"nome_parte\":nome_parte, \"classe_processo\":classe_processo, \"assunto_processo\":assunto_processo,\"data_local_distribuicao\":data_local_distribuicao,\"cod_empresa\":cod_empresa})\n",
    "\n",
    "               if linha:\n",
    "                  dados.append([nro_processo, link_processo,nome_parte,classe_processo,assunto_processo,data_local_distribuicao,cod_empresa])\n",
    "                  carrega_processo(linha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca em Todos os Foros \"-1\"\n",
    "scrapping_foro(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca Foro por Foro\n",
    "for foro in lista_foros: \n",
    "    scrapping_foro(foro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processos = pd.DataFrame(dados,columns=[\"nro_processo\",\"link_processo\",\"nome_parte\",\"classe_processo\",\"assunto_processo\",\"data_local_distribuicao\",\"cod_empresa\"])\n",
    "df_processos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_arquivo = str(time.strftime(\"%Y_%m_%d\")) + '_tjsp_processos_' + cod_empresa.lower() + '.csv'\n",
    "\n",
    "# Salva o arquivo \n",
    "df_processos.to_csv('./dados/' + nome_arquivo,sep=\";\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
